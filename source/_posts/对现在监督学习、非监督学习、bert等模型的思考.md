---
title: 对现在监督学习、非监督学习、bert等模型的思考
date: 2019-11-14 14:54:15
tags: [深度学习,思考]
---
不管是bert类的大规模预训练模型，还是视觉领域最新的把ImageNet准确率提升1%的Noisy Student，他们一个共同的特点，非监督学习。非监督学习的好处是数据大，通过大规模数据更容易学到其中的隐含规律、一般规律。
bert用了一种很巧妙的方法，开了个好头，一个新的思维方向。
数据量很多没法标注对把，我们可以把模型的结果设置称一种简单的关系，我们的目标是通过大量隐藏层去学到一个语言的内在关系。
bert采用的方法：
1，是使用两句话是否为上下句这种关系作为标签。这种关系不需要人工标注，通过简单的算法记录一下哪些是被分割开的就能实现。
2，还有一种方法是遮蔽部分词，让模型去预测，这种方法同样不需要标注数据，因为标签就是你遮蔽掉的词。
3，不仅仅做一种预测训练，既做了只有两个类别的上下句预测，又做了预测单词的非分类。我感觉这样使模型输出的结果不同，尽量减少对单一特定任务的效果，而是通过切换最后一层功能，尽量使模型学到的只是语言的特点，具体来说会在较深的层次中对各种词语搭配、句子骨干比较敏感，做出相应的反馈。
举个例子：
词语块：
```
解放碑吃火锅
农民辛苦 水稻
...
```
骨干类：
```
xxx 是 xxx 榜样
xxx 省 xxx 市
...
```

再说说Noisy Student，
它是以监督训练开始的，后续循环使用上一个老师预测的结果作为标签，可以看作是扩大规模的监督学习，因为后续不需要人工打标签，整体上可以看作是非监督学习。
在过程中加入了噪声数据，可以理解为是扩大数据类型，让模型学到对细节块的识别、敏感度。后期在正确的预测上一跑，之前对细节块的敏感性、识别能力可以用的到。当它能很好的识别细节后，一个funetuning之类的操作就很得到很好的结果。

### 总结
想办法提升模型对细节块的识别能力是提升准确率的一个方向。






老博客地址：[https://www.jianshu.com/u/1c73a3a8ae2d](https://www.jianshu.com/u/1c73a3a8ae2d)
新博客地址：[https://inspiring26.github.io/](https://inspiring26.github.io/)