---
title: 部署深度学习模型的总结与新思考
date: 2019-10-24 11:19:07
tags: [keras,pytorch,深度学习,思考]
---
使用异步非阻塞服务器可以同时响应较多的请求。
同时，可以把写操作先存内存数据库，每小时或其他时长写到硬盘一次。
tornado不用再使用WSGI服务器了，从基础上保证比较快。
如果是使用flask的话，还要使用一个WSGI服务器才能保证速度。

### 新想法
将图片矩阵传入redis，设立一个异步调用，当传入数据后就发起调用预测函数。
对预测函数来说，如果能读到数据就放入模型中跑，读不到数据说明在别的并发请求中已经将该数据处理了。

### 思考2
由于存入redis，数组需要序列化和反序列化，可能存放在一个全局变量中更快。

实际测试过以后，没有同时读取两个图片进行预测的情况，都是单个图片的。
难道是那几十个并发放在1000ms内被分的还是很散的？